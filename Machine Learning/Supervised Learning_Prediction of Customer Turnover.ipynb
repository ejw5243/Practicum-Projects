{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beta Bank Project\n",
    "\n",
    "## Purpose\n",
    "\n",
    "The purpose of this project is to develop a model that predicts whether a Beta Bank customer will leave the bank soon. The model will be trained based on data from previous clients' behavior and past terminations. The model must have an F1 Score of at least 0.59.\n",
    "\n",
    "## Table of Contents\n",
    "<a href='#General Data Information'>General Data Information</a>\n",
    "\n",
    "<a href='#Feature Preparation'>Feature Preparation</a>\n",
    "\n",
    "<a href='#Model Development'>Model Development</a>\n",
    "\n",
    "<a href='#Class Balance'>Class Balance</a>\n",
    "\n",
    "<a href='#Imbalanced Model Training'>Imbalanced Model Training</a>\n",
    "\n",
    "<a href='#Class Balancing'>Class Balancing</a>\n",
    "\n",
    "<a href='#Final Model Selection and Training'>Final Model Selection and Training</a>\n",
    "\n",
    "<a href='#Conclusion'>Conclusion</a>\n",
    "\n",
    "<a id='General Data Information'></a>\n",
    "## General Data Information\n",
    "\n",
    "Initially, a general look at the data is performed and the necessary sklearn modules are imported. Since the target attribute is categorical (customer either exited their contract or not), classification modules are imported for Decision Tree, Random Forest, and Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries and modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy as sc\n",
    "from scipy import stats as st\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, f1_score, roc_curve, roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "RowNumber          10000 non-null int64\n",
      "CustomerId         10000 non-null int64\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             9091 non-null float64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "dtypes: float64(3), int64(8), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8.0</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2.0</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0     2.0       0.00              1          1               1   \n",
       "1     1.0   83807.86              1          0               1   \n",
       "2     8.0  159660.80              3          1               0   \n",
       "3     1.0       0.00              2          0               0   \n",
       "4     2.0  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/datasets/Churn.csv')\n",
    "\n",
    "data.info()\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the initial look at the data, it can be seen that there are some missing values in the `Tenure` attribute. These missing values are filled in with the median value of the `Tenure` attribute since there doesn't seem to be a logical reason why they are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fills all missing values in Tenure column with the median value\n",
    "data['Tenure'].fillna(data['Tenure'].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Feature Preparation'></a>\n",
    "### Feature Preparation\n",
    "\n",
    "Out of the dataset attributes provided, there are three attributes that are the object type that must be converted to numerical features in order to use the classification methods to train the model. This will be accomplished by Label Encoding the dataset, which will convert all of the attributes to a numerical scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates instance of OrdinalEncoder class which is actually label encoding\n",
    "encoder = OrdinalEncoder()\n",
    "#Tunes and transforms data to a new data frame\n",
    "data_ordinal = pd.DataFrame(encoder.fit_transform(data), columns = data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the features are in a workable format for the classification methods, the data set can be separated into the features and target, and be separated into three different datasets: a training dataset, validation dataset, and testing dataset. The original dataset is split into a 3:1:1 ratio for the training, validating, and testing datasets, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splits the original dataset into 60% training, 40% leftover\n",
    "data_train, data_valid_test = train_test_split(data_ordinal, test_size = 0.4, random_state=12345)\n",
    "#Splits the leftover dataset into 50% validating, 50% testing for 20%/20% overall\n",
    "data_valid, data_test = train_test_split(data_valid_test, test_size = 0.5, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the goal of this model is to predict whether or not a customer will soon exit their contract with the bank, the `Exited` attribute is the target and the remaining attributes are the features. The size of each dataset is output to verify that each dataset contains the appropriate number of entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 13)\n",
      "(2000, 13)\n",
      "(2000, 13)\n"
     ]
    }
   ],
   "source": [
    "#Creates features and targets of each dataset\n",
    "features_train = data_train.drop('Exited', axis=1)\n",
    "target_train = data_train['Exited']\n",
    "print(features_train.shape)\n",
    "\n",
    "features_valid = data_valid.drop('Exited', axis=1)\n",
    "target_valid = data_valid['Exited']\n",
    "print(features_valid.shape)\n",
    "\n",
    "features_test = data_test.drop('Exited', axis=1)\n",
    "target_test = data_test['Exited']\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training, validation, and testing datasets are created, the appropriate sample size of each dataset is verified, and the targets and features defined for each. Since each of the features have different scales for their range of values, and thus different levels of importance to the model, the features for each dataset must be scaled to ensure equal importance. The numerical attributes that most affect whether a customer will exit their bank contract and must be standardized are `CreditScore`, `Age`, `Balance`, `NumOfProducts`, and `EstimatedSalary`. The data is tuned with the variance and standard deviation of the training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates list of all numeric features that must be scaled.\n",
    "numeric = ['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'EstimatedSalary']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features_train[numeric])\n",
    "\n",
    "features_train[numeric] = scaler.transform(features_train[numeric])\n",
    "features_valid[numeric] = scaler.transform(features_valid[numeric])\n",
    "features_test[numeric] = scaler.transform(features_test[numeric])\n",
    "\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Model Development'></a>\n",
    "## Model Development\n",
    "<a id='Class Balance'></a>\n",
    "### Class Balance\n",
    "\n",
    "Prior to training the model, the distribution of customers that left their bank contract vs. customer's that have not exited is determined to understand the balance of classes in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    4804\n",
       "1.0    1196\n",
       "Name: Exited, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Counts number of customers that have and have not exited.\n",
    "target_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately 80% of the training dataset consists of customers that have not left their bank contract which clearly represents a significant imbalance of classes. Initially, the model will be trained without fixing the imbalance. However, the imbalance will be fixed and the model retrained to determine how this impacts the quality of the model.\n",
    "<a id='Imbalanced Model Training'></a>\n",
    "### Imbalanced Model Training\n",
    "\n",
    "Three types of classification models will be trained with varying hyperparameters: Logistic Regression, Decision Tree, and Random Forest. The F1 Score will be calculated for each to help determine which model should be selected for the best quality. As previously mentioned, these models will be trained on the imbalanced data, so the results of these will be utilized more for determining the effect balancing the data has on the model quality.\n",
    "\n",
    "#### Logistic Regression Model\n",
    "\n",
    "For the logistic regression model, two hyperparameters will be altered: the `C` parameter (inverse regularization) and the `penalty` parameter. For the C parameter, a range of regularization values from very small (0.01) to very large (100) are selected, including 1 (neutral) to determine how it affects overfitting. For the penalty parameter, Lasso Regression (l1) and Ridge Regression (l2) are the two options evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best linear regression model has an F1 Score of 0.27, with the following hyperparameters: C Value: 100,  Penalty: l1\n"
     ]
    }
   ],
   "source": [
    "#Sets a baseline score to compare model F1 scores\n",
    "best_lr_score = 0\n",
    "best_lr_hypers = [0, 0]\n",
    "\n",
    "#Loops through 6 different c values and 2 different regression regularizations and outputs the F1 Score\n",
    "for c_value in [0.01, 0.1, 0.5, 1, 10, 100]:\n",
    "    for penalty in ['l1','l2']:\n",
    "        logistic_regression_model = LogisticRegression(random_state = 12345, penalty=penalty, C = c_value, solver = 'liblinear')\n",
    "        logistic_regression_model.fit(features_train, target_train)\n",
    "        logistic_regression_predicted = logistic_regression_model.predict(features_valid)\n",
    "        if f1_score(target_valid, logistic_regression_predicted) > best_lr_score:\n",
    "            best_lr_score = f1_score(target_valid, logistic_regression_predicted)\n",
    "            best_lr_hypers = [c_value, penalty]\n",
    "        \n",
    "print(\"The best linear regression model has an F1 Score of {:.2f}, with the following hyperparameters: C Value: {},  Penalty: {}\".format(best_lr_score, best_lr_hypers[0], best_lr_hypers[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Model\n",
    "\n",
    "For the Decision Tree model, only the `max_depth` hyperparameter will be altered from values between 2 and 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best decision tree model has an F1 Score of 0.55, with a max_depth of 4.\n"
     ]
    }
   ],
   "source": [
    "#Sets a baseline score to compare model F1 scores\n",
    "best_dt_score = 0\n",
    "best_dt_hyper = 0\n",
    "\n",
    "#Loops through a max_depth from 2 to 8. Calculates F1 Score for each\n",
    "for depth in range (2,9):\n",
    "    decision_tree_model = DecisionTreeClassifier(max_depth=depth, random_state=12345)\n",
    "    decision_tree_model.fit(features_train, target_train)\n",
    "    decision_tree_predicted = decision_tree_model.predict(features_valid)\n",
    "    if f1_score(target_valid, decision_tree_predicted) > best_dt_score:\n",
    "            best_dt_score = f1_score(target_valid, decision_tree_predicted)\n",
    "            best_dt_hyper = depth\n",
    "\n",
    "print(\"The best decision tree model has an F1 Score of {:.2f}, with a max_depth of {}.\".format(best_dt_score, best_dt_hyper))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model\n",
    "\n",
    "For the Random Forest model, two hyperparameters will be altered: the `n_estimators` parameter and the `max_depth` parameter. The number of trees (`n_estimators`) is varied from 10 to 50 in increments of 10. The `max_depth` parameter is varied from 2 to 10 in increments of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best random forest model has an F1 Score of 0.55, with the following hyperparameters: n_estimators: 40,  max_depth: 10.\n"
     ]
    }
   ],
   "source": [
    "#Sets a baseline score to compare model F1 scores\n",
    "best_rf_score = 0\n",
    "best_rf_hypers = [0, 0]\n",
    "\n",
    "#Loops through n_estimators of 10, 20, 30, 40, and 50, and max_depth of 2, 4, 6, 8, and 10. Calculates F1 Score for each\n",
    "for tree in range(10, 51, 10):\n",
    "    for depth in range(2, 11, 2):\n",
    "        random_forest_model = RandomForestClassifier(n_estimators = tree, max_depth = depth, random_state=12345)\n",
    "        random_forest_model.fit(features_train, target_train)\n",
    "        random_forest_predicted = random_forest_model.predict(features_valid)\n",
    "        if f1_score(target_valid, random_forest_predicted) > best_rf_score:\n",
    "            best_rf_score = f1_score(target_valid, random_forest_predicted)\n",
    "            best_rf_hypers = [tree, depth] \n",
    "    \n",
    "print(\"The best random forest model has an F1 Score of {:.2f}, with the following hyperparameters: n_estimators: {},  max_depth: {}.\".format(best_rf_score, best_rf_hypers[0], best_rf_hypers[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalance Class Model Conclusion\n",
    "\n",
    "Using the imbalanced training dataset resulted in a wide range of F1 Scores. The logistic regression model had the lowest F1 Scores ranging from 0.19 to 0.27. Since these scores are far from the minimum required scores, this type of model does not need to be retrained with balanced classes since it is not expected to make a significant enough difference to be considered acceptable. The decision tree model had much better F1 Scores ranging from 0.42 to 0.55, which are borderline acceptable. The decision tree model will be retrained after balancing the classes. The random forest model had a wide range of F1 Scores from 0.11 to 0.55. The highest F1 Scores were at a `max_depth` of 6 and greater, so this range of hyperparameters will be focused when this model is retrained after balancing the classes. \n",
    "\n",
    "<a id='Class Balancing'></a>\n",
    "### Class Balancing\n",
    "\n",
    "With a baseline F1 Score determined for the decision tree and random forest models of interest for the imbalanced class data, the balancing can be fixed to determine the effect on the F1 Score. Class balancing will be done by two methods: setting the `class_weight` argument to 'balanced', and upsampling the data. Downsampling the data is an alternate option; however, its effect is not expected to be much different than the upsampling method due to the nature of the functions used to alter the data. The hyperparameters resulting in the highest F1 Score as determined above will only be used for this evaluation.\n",
    "\n",
    "#### `class_weight`\n",
    "\n",
    "**Decision Tree Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth: 2   F1 Score: 0.54\n",
      "max_depth: 3   F1 Score: 0.54\n",
      "max_depth: 4   F1 Score: 0.52\n",
      "max_depth: 5   F1 Score: 0.58\n",
      "max_depth: 6   F1 Score: 0.55\n",
      "max_depth: 7   F1 Score: 0.52\n",
      "max_depth: 8   F1 Score: 0.52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loops through a max_depth from 2 to 8. Calculates F1 Score for each\n",
    "for depth in range (2,9):\n",
    "    decision_tree_model = DecisionTreeClassifier(max_depth=depth, class_weight = 'balanced', random_state=12345)\n",
    "    decision_tree_model.fit(features_train, target_train)\n",
    "    decision_tree_predicted = decision_tree_model.predict(features_valid)\n",
    "    print(\"max_depth:\",depth,\"  F1 Score: {:.2f}\".format(f1_score(target_valid, decision_tree_predicted)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators: 10   max_depth: 6   F1 Score: 0.57\n",
      "n_estimators: 10   max_depth: 8   F1 Score: 0.58\n",
      "n_estimators: 10   max_depth: 10   F1 Score: 0.56\n",
      "\n",
      "n_estimators: 20   max_depth: 6   F1 Score: 0.61\n",
      "n_estimators: 20   max_depth: 8   F1 Score: 0.60\n",
      "n_estimators: 20   max_depth: 10   F1 Score: 0.57\n",
      "\n",
      "n_estimators: 30   max_depth: 6   F1 Score: 0.61\n",
      "n_estimators: 30   max_depth: 8   F1 Score: 0.61\n",
      "n_estimators: 30   max_depth: 10   F1 Score: 0.57\n",
      "\n",
      "n_estimators: 40   max_depth: 6   F1 Score: 0.61\n",
      "n_estimators: 40   max_depth: 8   F1 Score: 0.61\n",
      "n_estimators: 40   max_depth: 10   F1 Score: 0.58\n",
      "\n",
      "n_estimators: 50   max_depth: 6   F1 Score: 0.60\n",
      "n_estimators: 50   max_depth: 8   F1 Score: 0.62\n",
      "n_estimators: 50   max_depth: 10   F1 Score: 0.58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loops through n_estimators of 10, 20, 30, 40, and 50, and max_depth of 6, 8, and 10. Calculates F1 Score for each\n",
    "for tree in range(10, 51, 10):\n",
    "    for depth in range(6, 11, 2):\n",
    "        random_forest_model = RandomForestClassifier(n_estimators = tree, class_weight = 'balanced', max_depth = depth, random_state=12345)\n",
    "        random_forest_model.fit(features_train, target_train)\n",
    "        random_forest_predicted = random_forest_model.predict(features_valid)\n",
    "        print(\"n_estimators:\", tree, '  max_depth:', depth, '  F1 Score: {:.2f}'.format(f1_score(target_valid, random_forest_predicted)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`class_weight` Conclusion**\n",
    "\n",
    "Setting the `class_weight` equal to balanced made a fairly noticeable difference in the F1 Score for both models. For the decision tree, the F1 Score at a `max_depth` of 5 improved to 0.58 from 0.52 and from 0.42 to 0.54 at a `max_depth` of 3. The other iterations had slight to no improvement in F1 Score. For the random forest model, F1 Score increased by 0.10 for nearly each iteration at a `max_depth` of 6 and 8. The highest F1 Score was 0.62.\n",
    "\n",
    "Based on this evaluation, balancing the classes is critical in developing a model with an acceptable F1 Score.\n",
    "\n",
    "#### Upsampling\n",
    "\n",
    "Upsampling of the data will be performed by making the rarer target occurrences, customer's who exited their contract in this case, more frequent. A function will be created to create a new training sample with an increased number of positive target occurrences, and then this training sample will be used to train the models of interest. The number of positive targets will be increased by a factor of 4 since the initial distribution for the rarer target occurrences was 20% of the overall dataset. To limit the required time to train the model and output the results, and better understand the effect of the repeat factor for upsampling only a `max_depth` of 5 and 6 is used for the decision tree and an `n_estimators` of 40 and 50 are used as these seem to represent the overall highest F1 Scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Purposes of this function is to separate the targets and features into TP, TN, FP and FN. TP features and targets will be multiplied\n",
    "# by the repeat factor to increase the occurrences of positive targets. The upsampled features and targets will be shuffled prior to\n",
    "#returning them to the model for training\n",
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    \n",
    "    features_upsampled, target_upsampled = shuffle(\n",
    "        features_upsampled, target_upsampled, random_state=12345)\n",
    "    \n",
    "    return features_upsampled, target_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsample Factor: 4 max_depth: 5   F1 Score: 0.58\n",
      "Upsample Factor: 4 max_depth: 6   F1 Score: 0.55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loops through a max_depth from 2 to 8 and a repeat factor of 4. Calculates F1 Score for each\n",
    "features_upsampled, target_upsampled = upsample(features_train, target_train, 4)\n",
    "for depth in range (5,7):\n",
    "    decision_tree_model = DecisionTreeClassifier(max_depth=depth, random_state=12345)\n",
    "    decision_tree_model.fit(features_upsampled, target_upsampled)\n",
    "    decision_tree_predicted = decision_tree_model.predict(features_valid)\n",
    "    print(\"Upsample Factor: 4\", \"max_depth:\",depth,\"  F1 Score: {:.2f}\".format(f1_score(target_valid, decision_tree_predicted)))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsample Factor: 4 n_estimators: 40   max_depth: 6   F1 Score: 0.60\n",
      "Upsample Factor: 4 n_estimators: 40   max_depth: 8   F1 Score: 0.59\n",
      "Upsample Factor: 4 n_estimators: 40   max_depth: 10   F1 Score: 0.61\n",
      "\n",
      "Upsample Factor: 4 n_estimators: 50   max_depth: 6   F1 Score: 0.61\n",
      "Upsample Factor: 4 n_estimators: 50   max_depth: 8   F1 Score: 0.60\n",
      "Upsample Factor: 4 n_estimators: 50   max_depth: 10   F1 Score: 0.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loops through n_estimators of 10, 20, 30, 40, and 50, a max_depth of 6, 8, and 10, and a repeat factor of 4. Calculates F1 Score for each\n",
    "features_upsampled, target_upsampled = upsample(features_train, target_train, 4)\n",
    "for tree in range(40, 51, 10):\n",
    "    for depth in range(6, 11, 2):\n",
    "        random_forest_model = RandomForestClassifier(n_estimators = tree, max_depth = depth, random_state=12345)\n",
    "        random_forest_model.fit(features_upsampled, target_upsampled)\n",
    "        random_forest_predicted = random_forest_model.predict(features_valid)\n",
    "        print(\"Upsample Factor: 4\", \"n_estimators:\", tree, '  max_depth:', depth, '  F1 Score: {:.2f}'.format(f1_score(target_valid, random_forest_predicted)))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Upsampling Conclusion**\n",
    "\n",
    "Upsampling the rarer target occurrences made a fairly noticeable difference in the F1 Score for both models, similar to the `class_weight` balancing. Both models improved to a F1 Score that were similar to the F1 Scores of the `class_weight` balancing models, with the upsampling seeming to have a more consistent increase for all variations of the hyperparameters.\n",
    "\n",
    "Based on this evaluation, balancing the classes is required in order to develop a model with an F1 Score that meets the requirements.\n",
    "\n",
    "<a id='Final Model Selection and Training'></a>\n",
    "## Final Model Selection and Training\n",
    "\n",
    "Based on the various model results from Section 4, it is crucial to balance the classes prior to training the model. Setting the `class_weight` equal to 'balanced' will be the selected method for balancing the classes due to the simplicity in integrating it into the selected model. From the various models and hyperparameters tested, the random forest model with 50 `n_estimators` and a `max_depth` of 8 resulted in the highest F1 Score. This model will be trained on the training and validation data and will be tested on the testing dataset to ensure it is acceptable for the bank's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines training and validation datasets back into one\n",
    "features_train_test = pd.concat([features_train, features_valid])\n",
    "target_train_test = pd.concat([target_train, target_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 Score of the model is 0.59.\n"
     ]
    }
   ],
   "source": [
    "#Calculate F1 Score of the Random Forest Model using the testing dataset. Hyperparameters of the most accurate model used.\n",
    "random_forest_model_test = RandomForestClassifier(n_estimators = 50, max_depth = 8, class_weight = 'balanced', random_state=12345)\n",
    "random_forest_model_test.fit(features_train_test, target_train_test)\n",
    "random_forest_model_test_predicted = random_forest_model_test.predict(features_test)\n",
    "print(\"The F1 Score of the model is {:.2}.\".format(f1_score(target_test, random_forest_model_test_predicted)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the F1 Score of the model meets the required value, the model will still be sanity checked against a random answer model by comparing the Receiver Operating Characteristic (ROC) curves and then calculating the area under curve (AUC) ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Calculates the ROC for the model for the testing data set\n",
    "probabilities_test = random_forest_model_test.predict_proba(features_test)\n",
    "probabilities_one_test = probabilities_test[:, 1]\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(target_test, probabilities_one_test)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "\n",
    "# ROC curve for random model\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC-ROC for the selected model is 0.848.\n"
     ]
    }
   ],
   "source": [
    "#Calculates AUC-ROC\n",
    "auc_roc = roc_auc_score(target_test, probabilities_one_test)\n",
    "\n",
    "print('The AUC-ROC for the selected model is {:.3f}.'.format(auc_roc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selected model has an AUC-ROC of 0.848 which is well above the AUC-ROC of the random answer model (0.5). The model has met the required F1 Score and has passed the sanity check.\n",
    "<a id='Conclusion'></a>\n",
    "## Conclusion\n",
    "\n",
    "The purpose of this project was to develop a model that predicts whether a Beta Bank customer will leave the bank soon. The model was trained based on data from previous clients' behavior and past terminations, split into three separate datasets: a training, validation, and testing dataset. The model quality was quantified based on the F1 Score.\n",
    "\n",
    "The model was initially trained with the raw dataset, which was severeley imbalanced. However, this resulted in an F1 Score well below the required value. Class balancing was fixed via two methods: setting the `class_weight` equal to 'balanced', and by upsampling the positive target data. Retraining the model with the balanced data increased the quality of the models and resulted in an acceptable F1 Score.\n",
    "\n",
    "From the various iterations of Random Forest and Decision Tree models trained and tested against the validation dataset, a Random Forest model with 40 `n_estimators` and a `max_depth` of 8 proved to have the highest F1 Score on the validation dataset. This specific model was trained on the combined training and validation datasets and then tested against the testing dataset, resulting in an acceptable F1 Score of 0.59. To confirm the model quality was accurate, the AUC-ROC was plotted and compared to a random answer model, which verified the model's quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
